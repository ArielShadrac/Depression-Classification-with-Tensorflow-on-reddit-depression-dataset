{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import tensorflow\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'datasets/depression_dataset_reddit_cleaned.csv'\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    texts = df['clean_text'].dropna().values\n",
    "    labels = df['is_depression'].values\n",
    "    print(df.shape)\n",
    "    print(texts.shape)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(texts, max_length=128):\n",
    "    \"\"\"Preprocess the text data using BERT preprocessing and encoding.\"\"\"\n",
    "    bert_processor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "    bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "    \n",
    "    batch_size = 32\n",
    "    encoded_texts = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        preprocessed = bert_processor(batch_texts)\n",
    "        encoded = bert_encoder(preprocessed)['pooled_output']\n",
    "        encoded_texts.append(encoded.numpy())\n",
    "\n",
    "    return np.vstack(encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tensorflow.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='relu', kernel_regularizer=tensorflow.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax1.set_title('Loss over Epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax2.set_title('Accuracy over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path):\n",
    "    \"\"\"Pipeline pour charger, entraîner et évaluer le modèle.\"\"\"\n",
    "\n",
    "    texts, labels = load_data(data_path)\n",
    "    \n",
    "\n",
    "    X = text_processing(texts)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "\n",
    "    if len(np.unique(y)) == 2:\n",
    "        print(\"Classes équilibrées:\", np.bincount(y))\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "\n",
    "    model = build_model(input_shape=(X.shape[1],))\n",
    "    \n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            tensorflow.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            tensorflow.keras.callbacks.ModelCheckpoint('depression_model.h5', save_best_only=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Évaluation\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\nRapport de classification :\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Non-dépression', 'Dépression']))\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"F1-Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7731, 2)\n",
      "(7731,)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf_keras.src.engine.base_layer_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/depression_dataset_reddit_cleaned.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m----> 3\u001b[0m     model, history \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pipeline pour charger, entraîner et évaluer le modèle.\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m texts, labels \u001b[38;5;241m=\u001b[39m load_data(data_path)\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtext_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "Cell \u001b[1;32mIn[76], line 3\u001b[0m, in \u001b[0;36mtext_processing\u001b[1;34m(texts, max_length)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtext_processing\u001b[39m(texts, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess the text data using BERT preprocessing and encoding.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     bert_processor \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     bert_encoder \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mKerasLayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\engine\\base_layer.py:750\u001b[0m, in \u001b[0;36mLayer.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    748\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m tf_inspect\u001b[38;5;241m.\u001b[39mgetfullargspec(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m    749\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(arg_names[\u001b[38;5;241m1\u001b[39m : \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], args)))\n\u001b[1;32m--> 750\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Layer, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m# For safety, we only rely on auto-configs for a small set of\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;66;03m# serializable types.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m supported_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\utils\\version_utils.py:51\u001b[0m, in \u001b[0;36m__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\utils\\generic_utils.py:555\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, item)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\utils\\generic_utils.py:546\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras.src.engine.base_layer_v1'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_path = \"datasets/depression_dataset_reddit_cleaned.csv\"  \n",
    "    model, history = main(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
